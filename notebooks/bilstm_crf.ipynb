{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model BiLSTM + CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchcrf import CRF  # Install via `pip install pytorch-crf`\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "import pickle\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model BILSTM+CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read file CSV and Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    sentences = []\n",
    "    labels = []\n",
    "    current_sentence = []\n",
    "    current_labels = []\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        token = row['token']\n",
    "        label = row['label']\n",
    "        \n",
    "        # Check if the row marks the end of a sentence (if applicable)\n",
    "        if pd.isna(token):  # Use NaN or other criteria to split sentences\n",
    "            if current_sentence:\n",
    "                sentences.append(current_sentence)\n",
    "                labels.append(current_labels)\n",
    "                current_sentence = []\n",
    "                current_labels = []\n",
    "        else:\n",
    "            current_sentence.append(token)\n",
    "            current_labels.append(label)\n",
    "\n",
    "    # Add the last sentence if it's not empty\n",
    "    if current_sentence:\n",
    "        sentences.append(current_sentence)\n",
    "        labels.append(current_labels)\n",
    "\n",
    "    return sentences, labels\n",
    "\n",
    "# Load train, test, and dev data\n",
    "train_sentences, train_labels = load_data('/Users/makchanna128gmail.com/Documents/CADT_CS/Y6/NLP/Project_text/NER_Project/data/cleaned_data/cleaned_data_train.csv')\n",
    "test_sentences, test_labels = load_data('/Users/makchanna128gmail.com/Documents/CADT_CS/Y6/NLP/Project_text/NER_Project/data/cleaned_data/cleaned_data_test.csv')\n",
    "dev_sentences, dev_labels = load_data('/Users/makchanna128gmail.com/Documents/CADT_CS/Y6/NLP/Project_text/NER_Project/data/cleaned_data/cleaned_data_dev.csv')\n",
    "\n",
    "print(f\"Loaded {len(train_sentences)} training sentences.\")\n",
    "print(f\"First training sentence: {train_sentences[0]}\")\n",
    "print(f\"First training sentence labels: {train_labels[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data_train = pd.read_csv('/Users/makchanna128gmail.com/Documents/CADT_CS/Y6/NLP/Project_text/NER_Project/data/cleaned_data/cleaned_data_train.csv')\n",
    "data_test = pd.read_csv('/Users/makchanna128gmail.com/Documents/CADT_CS/Y6/NLP/Project_text/NER_Project/data/cleaned_data/cleaned_data_test.csv')\n",
    "data_dev = pd.read_csv('/Users/makchanna128gmail.com/Documents/CADT_CS/Y6/NLP/Project_text/NER_Project/data/cleaned_data/cleaned_data_dev.csv')\n",
    "\n",
    "print(\"Data for training ========\\n\", data_train.shape)\n",
    "print(\"Data for test =========\\n\", data_test.shape)\n",
    "print(\"Data for dev ========\\n\", data_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for building a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build a vocabulary from the tokenized sentences\n",
    "def build_vocab(data, special_tokens=[\"<PAD>\", \"<UNK>\"]):\n",
    "    vocab = Counter([token for sentence in data for token in sentence])\n",
    "    word_to_idx = {token: idx for idx, (token, _) in enumerate(vocab.items(), start=len(special_tokens))}\n",
    "    \n",
    "    # Add special tokens\n",
    "    for idx, token in enumerate(special_tokens):\n",
    "        word_to_idx[token] = idx\n",
    "    \n",
    "    return word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build a tag vocabulary from the label sequences\n",
    "def build_tag_vocab(labels, special_tags=[\"<PAD>\"]):\n",
    "    unique_labels = set(tag for label_seq in labels for tag in label_seq)\n",
    "    tag_to_idx = {tag: idx for idx, tag in enumerate(unique_labels, start=len(special_tags))}\n",
    "    \n",
    "    # Add special tags\n",
    "    for idx, tag in enumerate(special_tags):\n",
    "        tag_to_idx[tag] = idx\n",
    "    \n",
    "    return tag_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabularies\n",
    "word_to_idx = build_vocab(train_sentences)\n",
    "tag_to_idx = build_tag_vocab(train_labels)\n",
    "\n",
    "print(f\"Word vocab size: {len(word_to_idx)}\")\n",
    "print(f\"Tag vocab size: {len(tag_to_idx)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to encode sentences and labels into numerical format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode sentences and labels into numerical format using vocabularies\n",
    "def encode_data(sentences, labels, word_to_idx, tag_to_idx):\n",
    "    encoded_sentences = [[word_to_idx.get(word, word_to_idx[\"<UNK>\"]) for word in sentence] for sentence in sentences]\n",
    "    encoded_labels = [[tag_to_idx[label] for label in label_seq] for label_seq in labels]\n",
    "    return encoded_sentences, encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the training, test, and dev datasets using the vocabularies built earlier\n",
    "train_sentences_encoded, train_labels_encoded = encode_data(train_sentences, train_labels, word_to_idx, tag_to_idx)\n",
    "test_sentences_encoded, test_labels_encoded = encode_data(test_sentences, test_labels, word_to_idx, tag_to_idx)\n",
    "dev_sentences_encoded, dev_labels_encoded = encode_data(dev_sentences, dev_labels, word_to_idx, tag_to_idx)\n",
    "\n",
    "# Print the first encoded sentence and its corresponding labels for verification\n",
    "print(f\"First encoded training sentence: {train_sentences_encoded[0]}\")\n",
    "print(f\"First encoded training labels: {train_labels_encoded[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to pad sentences and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to pad sentences and labels to the same length and create a mask\n",
    "def pad_data(sentences, labels, pad_idx=0):\n",
    "    sentences_padded = pad_sequence([torch.tensor(seq) for seq in sentences], batch_first=True, padding_value=pad_idx)\n",
    "    labels_padded = pad_sequence([torch.tensor(seq) for seq in labels], batch_first=True, padding_value=pad_idx)\n",
    "    mask = (sentences_padded != pad_idx)  # Create a mask for valid tokens\n",
    "    return sentences_padded, labels_padded, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the training, test, and dev datasets with the <PAD> token's index\n",
    "train_sentences_padded, train_labels_padded, train_mask = pad_data(train_sentences_encoded, train_labels_encoded, pad_idx=word_to_idx[\"<PAD>\"])\n",
    "test_sentences_padded, test_labels_padded, test_mask = pad_data(test_sentences_encoded, test_labels_encoded, pad_idx=word_to_idx[\"<PAD>\"])\n",
    "dev_sentences_padded, dev_labels_padded, dev_mask = pad_data(dev_sentences_encoded, dev_labels_encoded, pad_idx=word_to_idx[\"<PAD>\"])\n",
    "\n",
    "# Optional: Print the shapes of the padded sequences and masks to check the result\n",
    "print(f\"Shape of padded training sentences: {train_sentences_padded.shape}\")\n",
    "print(f\"Shape of padded training labels: {train_labels_padded.shape}\")\n",
    "print(f\"Shape of training mask: {train_mask.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defined Architecture of BiLSTM+CRF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BiLSTM-CRF Model for Sequence Labeling\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, tagset_size, embedding_dim, hidden_dim, pad_idx):\n",
    "        \"\"\"\n",
    "        Initializes the BiLSTM-CRF model with the specified parameters.\n",
    "\n",
    "        Args:\n",
    "        - vocab_size (int): Size of the vocabulary (number of unique words in the dataset).\n",
    "        - tagset_size (int): Number of unique tags (labels) in the dataset.\n",
    "        - embedding_dim (int): Dimension of word embeddings.\n",
    "        - hidden_dim (int): Number of features in the hidden state of the BiLSTM (should be divisible by 2 for bidirectional).\n",
    "        - pad_idx (int): Index used for padding in the word embeddings.\n",
    "        \"\"\"\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.tagset_size = tagset_size\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "        # 1. Embedding Layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        # 2. BiLSTM Layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        # 3. Linear Layer (maps LSTM output to tag space)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        # 4. CRF Layer\n",
    "        self.crf = CRF(tagset_size, batch_first=True)\n",
    "\n",
    "    def forward(self, sentences, tags, mask):\n",
    "        # 1. Embedding\n",
    "        embeddings = self.embedding(sentences)\n",
    "\n",
    "        # 2. BiLSTM\n",
    "        lstm_out, _ = self.lstm(embeddings)\n",
    "\n",
    "        # 3. Project LSTM outputs to tag space\n",
    "        emissions = self.hidden2tag(lstm_out)\n",
    "\n",
    "        # 4. CRF Loss (negative log likelihood)\n",
    "        loss = -self.crf(emissions, tags, mask=mask)\n",
    "        return loss\n",
    "\n",
    "    def predict(self, sentences, mask):\n",
    "        # 1. Embedding\n",
    "        embeddings = self.embedding(sentences)\n",
    "\n",
    "        # 2. BiLSTM\n",
    "        lstm_out, _ = self.lstm(embeddings)\n",
    "\n",
    "        # 3. Project LSTM outputs to tag space\n",
    "        emissions = self.hidden2tag(lstm_out)\n",
    "\n",
    "        # 4. CRF Decoding (predict best tag sequence)\n",
    "        predictions = self.crf.decode(emissions, mask=mask)\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EMBEDDING_DIM = 100  # Size of word embeddings\n",
    "HIDDEN_DIM = 256     # Size of LSTM hidden states\n",
    "VOCAB_SIZE = len(word_to_idx)\n",
    "TAGSET_SIZE = len(tag_to_idx)\n",
    "PAD_IDX = word_to_idx[\"<PAD>\"]\n",
    "\n",
    "# Initialize the model\n",
    "model = BiLSTM_CRF(vocab_size=VOCAB_SIZE, \n",
    "                   tagset_size=TAGSET_SIZE, \n",
    "                   embedding_dim=EMBEDDING_DIM, \n",
    "                   hidden_dim=HIDDEN_DIM, \n",
    "                   pad_idx=PAD_IDX)\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function create data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(sentences, labels, mask, batch_size=32):\n",
    "    \"\"\"\n",
    "    Creates a DataLoader for batching the input data during training, testing, or evaluation.\n",
    "\n",
    "    Args:\n",
    "    - sentences (Tensor): A tensor of padded sentences (size: batch_size x seq_length).\n",
    "    - labels (Tensor): A tensor of padded labels (size: batch_size x seq_length).\n",
    "    - mask (Tensor): A mask tensor that indicates the valid tokens in the sentences (size: batch_size x seq_length).\n",
    "    - batch_size (int, optional): The batch size to use when creating the DataLoader. Default is 32.\n",
    "\n",
    "    Returns:\n",
    "    - DataLoader: A DataLoader object that will load batches of data for training or evaluation.\n",
    "    \"\"\"\n",
    "    # Create a TensorDataset object that combines the sentences, labels, and mask tensors into a dataset.\n",
    "    # TensorDataset allows for efficient access to the tensors when iterating through the data.\n",
    "    dataset = TensorDataset(sentences, labels, mask)\n",
    "\n",
    "    # Create a DataLoader from the dataset. The DataLoader handles batching and shuffling of the data.\n",
    "    # shuffle=True means the data will be shuffled before each epoch, ensuring randomness in training.\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders for training, testing, and validation datasets\n",
    "train_dataloader = create_dataloader(train_sentences_padded, train_labels_padded, train_mask)\n",
    "test_dataloader = create_dataloader(test_sentences_padded, test_labels_padded, test_mask)\n",
    "dev_dataloader = create_dataloader(dev_sentences_padded, dev_labels_padded, dev_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store losses for plotting\n",
    "train_losses = []  # To store training losses at each epoch\n",
    "val_losses = []    # To store validation losses at each epoch\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Define learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
    "\n",
    "# Training loop\n",
    "EPOCHS = 40 # Set the number of epochs for training\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()  # Set model to training mode\n",
    "    total_train_loss = 0 # Variable to accumulate training loss\n",
    "\n",
    "    # Training phase\n",
    "    for sentences, tags, mask in train_dataloader:\n",
    "        # Move data to the GPU (if available) for faster processing\n",
    "        sentences, tags, mask = sentences.to(device), tags.to(device), mask.to(device)\n",
    "        # Zero the gradients from the previous step\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass: compute the loss\n",
    "        loss = model(sentences, tags, mask)\n",
    "        # Backward pass: compute gradients\n",
    "        loss.backward()\n",
    "        # Update the model's parameters\n",
    "        optimizer.step()\n",
    "        # Accumulate training loss\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    total_val_loss = 0 # Variable to accumulate validation loss\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient computation during evaluation\n",
    "        for sentences, tags, mask in dev_dataloader:\n",
    "            # Move data to the GPU (if available)\n",
    "            sentences, tags, mask = sentences.to(device), tags.to(device), mask.to(device)\n",
    "            # Forward pass for validation data\n",
    "            val_loss = model(sentences, tags, mask)\n",
    "            # Accumulate validation loss\n",
    "            total_val_loss += val_loss.item()\n",
    "            \n",
    "    # Calculate the average training and validation losses for the epoch\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    avg_val_loss = total_val_loss / len(dev_dataloader)\n",
    "\n",
    "    # Update learning rate scheduler\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Store losses for plotting\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    # Print losses\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"  Training Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"  Validation Loss: {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot graph training and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training and validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss', color='blue')\n",
    "plt.plot(val_losses, label='Validation Loss', color='red')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Function to evaluate the performance of the BiLSTM-CRF model\n",
    "def evaluate(model, dataloader, tag_to_idx):\n",
    "    \"\"\"\n",
    "    Function to evaluate the performance of the BiLSTM-CRF model on a given dataloader.\n",
    "    It computes the predictions and compares them with the true labels using a classification report.\n",
    "\n",
    "    Args:\n",
    "    - model: The trained BiLSTM-CRF model.\n",
    "    - dataloader: DataLoader containing the evaluation data (e.g., validation or test set).\n",
    "    - tag_to_idx: Dictionary mapping tags to their corresponding indices.\n",
    "    \"\"\"\n",
    "    # Convert tag indices to tags for later reference in evaluation\n",
    "    idx_to_tag = {idx: tag for tag, idx in tag_to_idx.items()}\n",
    "    # Set the model to evaluation mode (disables dropout, batch norm, etc.)\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize lists to store all predictions and true labels\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Disable gradient calculation to save memory and computation during evaluation\n",
    "    with torch.no_grad():\n",
    "        # Iterate over batches in the dataloader\n",
    "        for sentences, tags, mask in dataloader:\n",
    "            # Move data to GPU (if available)\n",
    "            sentences, tags, mask = sentences.to(device), tags.to(device), mask.to(device)\n",
    "\n",
    "            # Get predictions\n",
    "            predictions = model.predict(sentences, mask)\n",
    "\n",
    "            # Flatten predictions and labels\n",
    "            for pred, true, m in zip(predictions, tags, mask):\n",
    "                # Remove padding tokens from both predictions and true labels\n",
    "                pred = pred[:m.sum()]  # Remove padding predictions\n",
    "                true = true[:m.sum()]  # Remove padding labels\n",
    "                 # Convert predictions and true labels to their string tag equivalents\n",
    "                all_preds.extend([idx_to_tag[p] for p in pred]) # Convert predicted indices to tags\n",
    "                all_labels.extend([idx_to_tag[t.item()] for t in true]) # Convert true indices to tags\n",
    "\n",
    "    # Print out the classification report (precision, recall, F1 score for each tag)\n",
    "    print(classification_report(all_labels, all_preds))\n",
    "\n",
    "# Evaluate on dev set\n",
    "evaluate(model, dev_dataloader, tag_to_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the training loop finishes, save the model's state_dict to a file\n",
    "# Save the model's state_dict (parameters) to a file (with .plk extension)\n",
    "torch.save(model.state_dict(), \"bilstm_crf_model.plk\")\n",
    "print(\"Model saved as bilstm_crf_model.plk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model again with the same architecture\n",
    "loaded_model = BiLSTM_CRF(vocab_size=len(word_to_idx), \n",
    "                           tagset_size=len(tag_to_idx), \n",
    "                           embedding_dim=100, \n",
    "                           hidden_dim=256, \n",
    "                           pad_idx=word_to_idx[\"<PAD>\"])\n",
    "\n",
    "# Load the model's parameters from the saved file\n",
    "loaded_model.load_state_dict(torch.load('bilstm_crf_model.plk'), strict=False)\n",
    "\n",
    "# Move the loaded model to the appropriate device (CPU or GPU)\n",
    "loaded_model.to(device)\n",
    "# Print to testing\n",
    "print(\"Model loaded successfully\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
